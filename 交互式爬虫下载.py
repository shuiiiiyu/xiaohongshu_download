import csv
import os
import re
import time
import requests
from DrissionPage import ChromiumPage

# ===================== 1. é…ç½®å‚æ•° =====================
csv_file = "manual_scraped_data.csv"
ROOT_DOWNLOAD_DIR = 'manual_downloads'  # èµ„æºä¿å­˜æ ¹ç›®å½•

fieldnames = ["post_id", "title", "content", "create_at", "user_id", "liked_count",
              "cover_url", "post_url", "image_urls", "video_url"]

DOWNLOAD_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)",
    "Referer": "https://www.xiaohongshu.com/"
}

# ===================== 2. åŠŸèƒ½å·¥å…·å‡½æ•° =====================

def initialize_csv():
    """åˆå§‹åŒ–CSVæ–‡ä»¶å’Œä¸‹è½½ç›®å½•"""
    if not os.path.exists(ROOT_DOWNLOAD_DIR):
        os.makedirs(ROOT_DOWNLOAD_DIR)
    if not os.path.exists(csv_file):
        with open(csv_file, mode='a', newline='', encoding='utf-8-sig') as file:
            csv.DictWriter(file, fieldnames=fieldnames).writeheader()

def download_file(url, save_path):
    """é€šç”¨çš„ä¸‹è½½å‡½æ•°"""
    if not url or os.path.exists(save_path):
        return True
    try:
        response = requests.get(url, headers=DOWNLOAD_HEADERS, stream=True, timeout=15)
        response.raise_for_status()
        with open(save_path, 'wb') as file:
            for chunk in response.iter_content(chunk_size=8192):
                file.write(chunk)
        return True
    except Exception as e:
        print(f"   âŒ ä¸‹è½½å¤±è´¥: {url[:50]}... é”™è¯¯: {e}")
        return False

def save_assets(post_id, cover_url, image_urls, video_url):
    """ä¸ºæ¯ä¸ªå¸–å­åˆ›å»ºæ–‡ä»¶å¤¹å¹¶ä¿å­˜èµ„æº"""
    post_folder = os.path.join(ROOT_DOWNLOAD_DIR, str(post_id))
    os.makedirs(post_folder, exist_ok=True)

    # 1. ä¸‹è½½å°é¢
    if cover_url:
        download_file(cover_url, os.path.join(post_folder, "cover.jpg"))
    
    # 2. ä¸‹è½½å›¾é›†
    for i, img_url in enumerate(image_urls):
        download_file(img_url, os.path.join(post_folder, f"image_{i+1}.jpg"))
    
    # 3. ä¸‹è½½è§†é¢‘
    if video_url:
        download_file(video_url, os.path.join(post_folder, "video.mp4"))

# ===================== 3. æ ¸å¿ƒç›‘å¬ä¸»ç¨‹åº =====================

def run_manual_scraper():
    initialize_csv()
    
    # åˆå§‹åŒ–æµè§ˆå™¨ï¼Œä¼šè‡ªåŠ¨æ‰“å¼€ä¸€ä¸ªçª—å£
    page = ChromiumPage()
    
    print(f"\n{'='*60}")
    print("ğŸš€ æ‰‹åŠ¨äº¤äº’æŠ“å–æ¨¡å¼å·²å¯åŠ¨ï¼")
    print("ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­æ­£å¸¸æµè§ˆå°çº¢ä¹¦ï¼ˆå»ºè®®å…ˆç™»å½•ï¼‰ã€‚")
    print("2. åªè¦ä½ ã€é¼ æ ‡ç‚¹å‡»ã€‘è¿›å…¥ä»»ä½•ä¸€ä¸ªå¸–å­è¯¦æƒ…é¡µï¼Œç¨‹åºå°±ä¼šè‡ªåŠ¨é‡‡é›†ã€‚")
    print("3. å›¾ç‰‡å’Œè§†é¢‘å°†è‡ªåŠ¨ä¸‹è½½åˆ° 'manual_downloads' æ–‡ä»¶å¤¹ã€‚")
    print("4. æ§åˆ¶å°ä¼šå®æ—¶æ˜¾ç¤ºæŠ“å–è¿›åº¦ã€‚")
    print(f"{'='*60}\n")

    # å¼€å¯æ•°æ®åŒ…ç›‘å¬
    page.listen.start('https://edith.xiaohongshu.com/api/sns/web/v1/feed')

    scraped_ids = set()

    try:
        while True:
            # æŒç»­ç­‰å¾…æ•°æ®åŒ…å“åº”
            res = page.listen.wait(timeout=1) 
            
            if res:
                try:
                    raw = res.response.body
                    if 'data' in raw and raw['data']['items']:
                        info = raw['data']['items'][0]['note_card']
                        post_id = info.get('note_id', '')

                        if post_id and post_id not in scraped_ids:
                            # 1. åŸºç¡€ä¿¡æ¯è§£æ
                            title = info.get('title', 'æ— æ ‡é¢˜').strip()
                            # æ¸…æ´—æ­£æ–‡ä¸­çš„æ¢è¡Œç¬¦
                            content = re.sub(r'\s+', ' ', info.get('desc', '')).strip()
                            create_at = info.get('time', '')
                            user_id = info.get('user', {}).get('user_id', '')
                            liked_count = info.get('interact_info', {}).get('liked_count', '0')
                            
                            # 2. åª’ä½“èµ„æºè§£æ
                            image_list = info.get('image_list', [])
                            cover_url = image_list[0].get('url_default', '') if image_list else ''
                            image_urls = [i.get('url_default', '') for i in image_list]
                            
                            video_url = ''
                            if info.get('type') == 'video':
                                stream = info.get('video', {}).get('media', {}).get('stream', {})
                                # ä¼˜å…ˆå°è¯•æœ€é«˜ç”»è´¨é“¾æ¥
                                v_s = stream.get('h264') or stream.get('h265')
                                if v_s: 
                                    video_url = v_s[0].get('master_url') or v_s[0].get('backup_urls', [''])[0]

                            # 3. å†™å…¥CSVè®°å½•
                            row = {
                                "post_id": post_id, "title": title, "content": content,
                                "create_at": create_at, "user_id": user_id, "liked_count": liked_count,
                                "cover_url": cover_url, "post_url": page.url, 
                                "image_urls": image_urls, "video_url": video_url
                            }

                            with open(csv_file, mode='a', newline='', encoding='utf-8-sig') as f:
                                csv.DictWriter(f, fieldnames=fieldnames).writerow(row)
                            
                            # 4. æ‰§è¡Œå¼‚æ­¥ä¸‹è½½ä»»åŠ¡
                            print(f"ğŸ“Œ å‘ç°æ–°å¸–å­: {title[:15]}... | å¼€å§‹ä¸‹è½½èµ„æº...")
                            save_assets(post_id, cover_url, image_urls, video_url)
                            
                            scraped_ids.add(post_id)
                            print(f"âœ… å¤„ç†å®Œæˆ: {post_id}")
                
                except Exception as e:
                    print(f"âš ï¸ è§£ææ•°æ®åŒ…æ—¶å‡ºé”™: {e}")
            
            # é™ä½CPUå ç”¨
            time.sleep(0.1)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²ç”±ç”¨æˆ·åœæ­¢ï¼Œæ•°æ®å·²å®‰å…¨ä¿å­˜ã€‚")
    finally:
        page.quit()

if __name__ == "__main__":
    run_manual_scraper()